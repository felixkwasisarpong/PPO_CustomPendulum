# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mZxfTJxMJgzt04Cl9BjS7pzWfoBwalro
"""

#Install dependencies for Colab
!apt-get install -y xvfb ffmpeg
!pip install gym Box2D PyOpenGL PyOpenGL_accelerate pyvirtualdisplay

#virtual display
from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()

# Import required libraries
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gym
import matplotlib.pyplot as plt""

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#  Policy and Critic Networks
class PolicyNetwork(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(PolicyNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.mu = nn.Linear(64, output_dim)
        self.log_std = nn.Parameter(torch.zeros(1, output_dim))

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        mu = self.mu(x)
        std = torch.exp(self.log_std)
        return mu, std

class CriticNetwork(nn.Module):
    def __init__(self, input_dim):
        super(CriticNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.value = nn.Linear(64, 1)

    def forward(self, state):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        value = self.value(x)
        return value

#  Replay Buffer
class ReplayBuffer:
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []
        self.values = []
        self.advantages = []

    def store(self, state, action, reward, log_prob, value):
        self.states.append(np.array(state, dtype=np.float32).reshape(-1))
        self.actions.append(np.array(action, dtype=np.float32).reshape(-1))
        self.rewards.append(float(reward))
        self.log_probs.append(log_prob.item())
        self.values.append(value.item())

    def compute_advantages(self, gamma, lam, next_value):
        returns = []
        advantages = []
        gae = 0

        for i in reversed(range(len(self.rewards))):
            delta = self.rewards[i] + gamma * next_value - self.values[i]
            gae = delta + gamma * lam * gae
            advantages.insert(0, gae)
            next_value = self.values[i]
            returns.insert(0, gae + self.values[i])

        self.advantages = advantages
        self.returns = returns

    def clear(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []
        self.values = []
        self.advantages = []

#  PPOAgent Class
class PPOAgent:
    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, lam=0.95, clip_ratio=0.2, epochs=10):
        self.policy_net = PolicyNetwork(state_dim, action_dim).to(device)
        self.critic_net = CriticNetwork(state_dim).to(device)
        self.optimizer = optim.Adam(list(self.policy_net.parameters()) + list(self.critic_net.parameters()), lr=lr)
        self.gamma = gamma
        self.lam = lam
        self.clip_ratio = clip_ratio
        self.epochs = epochs
        self.buffer = ReplayBuffer()

    def select_action(self, state):
        state = torch.tensor(state, dtype=torch.float32).to(device)
        mu, std = self.policy_net(state)
        dist = torch.distributions.Normal(mu, std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum()
        value = self.critic_net(state)
        return action.cpu().detach().numpy(), log_prob.detach(), value.detach()

    def store_transition(self, state, action, reward, log_prob, value):
        self.buffer.store(state, action, reward, log_prob, value)

    def update(self, next_state):
        next_state = torch.tensor(next_state, dtype=torch.float32).to(device)
        next_value = self.critic_net(next_state).detach()
        self.buffer.compute_advantages(self.gamma, self.lam, next_value)

        states = torch.tensor(np.array(self.buffer.states), dtype=torch.float32).to(device)
        actions = torch.tensor(np.array(self.buffer.actions), dtype=torch.float32).to(device)
        returns = torch.tensor(self.buffer.returns, dtype=torch.float32).to(device)
        advantages = torch.tensor(self.buffer.advantages, dtype=torch.float32).to(device)
        old_log_probs = torch.tensor(np.array(self.buffer.log_probs), dtype=torch.float32).to(device)

        for _ in range(self.epochs):
            mu, std = self.policy_net(states)
            dist = torch.distributions.Normal(mu, std)
            new_log_probs = dist.log_prob(actions).sum(dim=-1)
            ratio = torch.exp(new_log_probs - old_log_probs)

            clipped_ratio = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)
            surrogate_loss = torch.min(ratio * advantages, clipped_ratio * advantages)
            policy_loss = -surrogate_loss.mean()

            values = self.critic_net(states).squeeze()
            value_loss = ((returns - values) ** 2).mean()

            loss = policy_loss + 0.5 * value_loss

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

        self.buffer.clear()

#  Train the Agent and Save Output
def train():
    env = gym.make('Pendulum-v1', new_step_api=True)
    agent = PPOAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])
    num_episodes = 1000
    max_timesteps = 200
    rewards_history = []

    #save the output
    with open('rewards_log.txt', 'w') as file:
        for episode in range(num_episodes):
            state = env.reset()
            state = np.array(state, dtype=np.float32).reshape(-1)
            episode_reward = 0

            for t in range(max_timesteps):
                action, log_prob, value = agent.select_action(state.reshape(1, -1))
                action = action.clip(env.action_space.low, env.action_space.high)

                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated

                reward = float(reward) if np.isscalar(reward) else reward.item()

                agent.store_transition(state, action, reward, log_prob, value)
                episode_reward += reward

                if done or t == max_timesteps - 1:
                    agent.update(next_state.reshape(1, -1))
                    break

                state = np.array(next_state, dtype=np.float32).reshape(-1)

            rewards_history.append(episode_reward)
            log_entry = f"Episode {episode}, Reward: {episode_reward}\n"
            print(log_entry)
            file.write(log_entry)

    # Plotting learning curve
    plt.plot(rewards_history)
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title('Learning Curve')
    plt.show()

train()